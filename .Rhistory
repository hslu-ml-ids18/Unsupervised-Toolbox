perplexity= input$Perplexity, verbose=TRUE,
max_iter = input$Iteration)
plot(tsne$Y)
text(tsne$Y, labels=data_label) })
# Run the application
shinyApp(ui = ui, server = server)
#
# This is a Shiny web application. You can run the application by clicking
# the 'Run App' button above.
#
# Find out more about building applications with Shiny here:
#
#    http://shiny.rstudio.com/
#
library(shiny)
# Define UI for application that draws a histogram
ui <- fluidPage(
# Application title
h1("Machine Learning 2 - Unsupervised Toolbox"),
"by",
strong("Giuliano Ardesi, Lisa Becker, Anastasiia Chebatarova, Axel Kandel, Alexander Kushe"),
# Sidebar with a slider input for number of bins
sidebarLayout(
sidebarPanel(
sliderInput("Perplexity",
"Number of Perplexitys:",
min = 2,
max = 100,
value = 10),
sliderInput("Epsilon",
"Number of Epsilons:",
min = 2,
max = 100,
value = 5),
sliderInput("Iteration",
"Number of Iterations:",
min = 2,
max = 20,
value = 5),
headerPanel('k-means clustering'),
sidebarPanel(
selectInput('xcol', 'X Variable', names(data)),
selectInput('ycol', 'Y Variable', names(data),
selected=names(data)[[2]]),
numericInput('clusters', 'Cluster count', 3,
min = 1, max = 9)
)
),
# Show a plot of the generated distribution
mainPanel(
plotOutput('tsne_plot'),
#  k-means clustering plot
plotOutput('plot1'),
# Absolutely-positioned panels
plotOutput('plot2', height = "800px", width = "900px")
)
)
)
# Define server logic required to draw a histogram
server <- function(input, output) {
# Combine the selected variables into a new data frame
selectedData <- reactive({
data[, c(input$xcol, input$ycol)]
})
clusters <- reactive({
kmeans(selectedData(), input$clusters)
})
output$plot1 <- renderPlot({
palette(c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3",
"#FF7F00", "#FFFF33", "#A65628", "#F781BF", "#999999"))
par(mar = c(5.1, 4.1, 0, 1))
plot(selectedData(),
col = clusters()$cluster,
pch = 20, cex = 3)
points(clusters()$centers, pch = 4, cex = 4, lwd = 4)
})
output$plot2 <- renderPlot({
mtscaled <- as.matrix(scale(data))
heatmap(mtscaled,
col = topo.colors(200, alpha=0.5),
Colv=F, scale="none")
})
# Create a PCA plot of the dataset
output$tsne_plot <- renderPlot({
# Define a function to run tsna with input variables
###t-Distributed Stochastic Neighbor Embedding (tSNE)
library(Rtsne)
# Use table row names to label the datapoint later in the plot:
data_label<-as.factor(rownames(data))
#remove duplicates:
data_unique <- unique(data)
#  Run tSNE:
tSNEdata <- as.matrix(scale(data_unique))
tsne <- Rtsne(tSNEdata, dims = 2,
perplexity= input$Perplexity, verbose=TRUE,
max_iter = input$Iteration)
plot(tsne$Y)
text(tsne$Y, labels=data_label) })
}
# Run the application
shinyApp(ui = ui, server = server)
#
# This is a Shiny web application. You can run the application by clicking
# the 'Run App' button above.
#
# Find out more about building applications with Shiny here:
#
#    http://shiny.rstudio.com/
#
library(shiny)
# Define UI for application that draws a histogram
ui <- fluidPage(
# Application title
h1("Machine Learning 2 - Unsupervised Toolbox"),
"by",
strong("Giuliano Ardesi, Lisa Becker, Anastasiia Chebatarova, Axel Kandel, Alexander Kushe"),
# Sidebar with a slider input for number of bins
sidebarLayout(
sidebarPanel(
sliderInput("Perplexity",
"Number of Perplexitys:",
min = 2,
max = 100,
value = 10),
sliderInput("Epsilon",
"Number of Epsilons:",
min = 2,
max = 100,
value = 5),
sliderInput("Iteration",
"Number of Iterations:",
min = 2,
max = 20,
value = 5),
headerPanel('k-means clustering'),
sidebarPanel(
selectInput('xcol', 'X Variable', names(data)),
selectInput('ycol', 'Y Variable', names(data),
selected=names(data)[[2]]),
numericInput('clusters', 'Cluster count', 3,
min = 1, max = 9)
)
),
# Show a plot of the generated distribution
mainPanel(
plotOutput('tsne_plot'),
#  k-means clustering plot
plotOutput('plot1'),
# Absolutely-positioned panels
plotOutput('plot2', height = "800px", width = "900px")
)
)
)
# Define server logic required to draw a histogram
server <- function(input, output) {
# Combine the selected variables into a new data frame
selectedData <- reactive({
data[, c(input$xcol, input$ycol)]
})
clusters <- reactive({
kmeans(selectedData(), input$clusters)
})
output$plot1 <- renderPlot({
palette(c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3",
"#FF7F00", "#FFFF33", "#A65628", "#F781BF", "#999999"))
par(mar = c(5.1, 4.1, 0, 1))
plot(selectedData(),
col = clusters()$cluster,
pch = 20, cex = 3)
points(clusters()$centers, pch = 4, cex = 4, lwd = 4)
})
output$plot2 <- renderPlot({
mtscaled <- as.matrix(scale(data))
heatmap(mtscaled,
col = topo.colors(200, alpha=0.5),
Colv=F, scale="none")
})
# Create a PCA plot of the dataset
output$tsne_plot <- renderPlot({
# Define a function to run tsna with input variables
###t-Distributed Stochastic Neighbor Embedding (tSNE)
library(Rtsne)
# Use table row names to label the datapoint later in the plot:
data_label<-as.factor(rownames(data))
#remove duplicates:
data_unique <- unique(data)
#  Run tSNE:
tSNEdata <- as.matrix(scale(data_unique))
tsne <- Rtsne(tSNEdata, dims = 2,
perplexity= input$Perplexity, verbose=TRUE,
max_iter = input$Iteration)
plot(tsne$Y)
text(tsne$Y, labels=data_label) })
}
# Run the application
shinyApp(ui = ui, server = server)
# Check training progress
plot(som_model, type="chances")
?kohonen
?kohonen
install.packages("kohonen")
library(kohonen)
raw = read.csv2("data.csv", header = TRUE, stringsAsFactors = F, dec = ".")
data = raw[,3:length(raw)]
library(kohonen)
# For plotting evaluation against colorcode # category (~ classification solution)
row_label <- as.factor(rownames(data)) colors <- c("red", "black", "blue")
# For plotting evaluation against colorcode # category (~ classification solution)
row_label <- as.factor(rownames(data))
colors <- c("red", "black", "blue")
colors <- colors[as.numeric(iris$Species)]
data_train_matrix <- as.matrix(scale(data))
# Define the neuronal grid
som_grid <- somgrid(xdim = 4, ydim = 4, topo="hexagonal")
# Train the model
som_model <- som(data_train_matrix, grid=som_grid, rlen=1000, alpha=c(0.05,0.01), keep.data=TRUE)
# Check training progress
plot(som_model, type="chances")
# Check training progress
plot(som_model, type="chances")
# Check training progress
plot(som_model, type="changes")
# Check how many samples are mapped to each node on the map
plot(som_model, type = "count")
plot(som_model,type = "mapping",
col=colors[row_label])
plot(som_model, type ="mapping",
labels = (rownames(data)),
col=colors[row_label])
# U-Matrix: measure of distance between each node and its neighbours.
# (Euclidean distance between weight vectors of neighboring neurons)
# Can be used to identify clusters/boundaries within the SOM map.
# Areas of low neighbour distance ~ groups of nodes that are similar. plot(som_model, type="dist.neighbours")
# Codes / Weight vectors: representative of the samples mapped to a node. # highlights patterns in the distribution of samples and variables. plot(som_model, type="codes")
# Heatmaps: identify interesting areas on the map.
# Visualise the distribution of a single variable (defined in [,x])
# across the map
# colnames(data) # to check index to put in [,x]
plot(som_model, type = "property", property = getCodes(som_model, 1)[,2]) # → Make a loop over all the variables ?
# Same as above but with original, unscaled data (can also be useful) var_unscaled <- aggregate(as.numeric(iris_unique[,1]),
by=list(som_model$unit.classif),FUN=mean, simplify=TRUE)[,2]
# Same as above but with original, unscaled data (can also be useful) var_unscaled <- aggregate(as.numeric(iris_unique[,1]),
by=list(som_model$unit.classif)
FUN=mean, simplify=TRUE)[,2]
# Same as above but with original, unscaled data (can also be useful)
var_unscaled <- aggregate(as.numeric(iris_unique[,1]),by=list(som_model$unit.classif)FUN=mean, simplify=TRUE)[,2]
# Same as above but with original, unscaled data (can also be useful)
var_unscaled <- aggregate(as.numeric(iris_unique[,1]),
by=list(som_model$unit.classif),
FUN=mean, simplify=TRUE)[,2]
# Clustering: isolate groups of samples with similar metrics
tree <- as.dendrogram(hclust(dist(as.numeric(unlist(som_model$codes))))) plot(tree, ylab = "Height (h)")
# Clustering: isolate groups of samples with similar metrics
tree <- as.dendrogram(hclust(dist(as.numeric(unlist(som_model$codes)))))
plot(tree, ylab = "Height (h)")
# Clustering: isolate groups of samples with similar metrics
tree <- as.dendrogram(hclust(dist(as.numeric(unlist(som_model$codes)))))
plot(tree, ylab = "Height (h)")
# Cut the tree somewhere based on the above tree
som_cluster <- cutree(hclust(dist(as.numeric(unlist(som_model$codes)))), h=2) # k groups or at h hight
# Visualize mapping based on HC
pretty_palette <- c("#1f77b4", '#ff7f0e', '#2ca02c', '#d62728',
'#9467bd', '#8c564b', '#e377c2')
plot(som_model, type="mapping",label=(rownames(data)),
bgcol=pretty_palette[som_cluster], col=colors[row_label])
add.cluster.boundaries(som_model,som_cluster)
View(data)
View(data)
data = read.csv2("data.csv", header = TRUE, stringsAsFactors = F, dec = ".")
View(data)
View(data)
row_label <- as.factor(rownames(data))
colors <- c("red", "black", "blue")
colors <- colors[as.numeric(iris$Species)]
data_train_matrix <- as.matrix(scale(data))
# Define the neuronal grid
som_grid <- somgrid(xdim = 4, ydim = 4, topo="hexagonal")
# Train the model
som_model <- som(data_train_matrix, grid=som_grid, rlen=1000, alpha=c(0.05,0.01), keep.data=TRUE)
# Check training progress
plot(som_model, type="changes")
# Check how many samples are mapped to each node on the map
plot(som_model, type = "count")
plot(som_model,type = "mapping",
col=colors[row_label])
plot(som_model, type ="mapping",
labels = (rownames(data)),
col=colors[row_label])
# U-Matrix: measure of distance between each node and its neighbours.
# (Euclidean distance between weight vectors of neighboring neurons)
# Can be used to identify clusters/boundaries within the SOM map.
# Areas of low neighbour distance ~ groups of nodes that are similar. plot(som_model, type="dist.neighbours")
# Codes / Weight vectors: representative of the samples mapped to a node. # highlights patterns in the distribution of samples and variables. plot(som_model, type="codes")
# Heatmaps: identify interesting areas on the map.
# Visualise the distribution of a single variable (defined in [,x])
# across the map
# colnames(data) # to check index to put in [,x]
plot(som_model, type = "property", property = getCodes(som_model, 1)[,2]) # → Make a loop over all the variables ?
# Clustering: isolate groups of samples with similar metrics
tree <- as.dendrogram(hclust(dist(as.numeric(unlist(som_model$codes)))))
plot(tree, ylab = "Height (h)")
# Cut the tree somewhere based on the above tree
som_cluster <- cutree(hclust(dist(as.numeric(unlist(som_model$codes)))), h=2) # k groups or at h hight
# Visualize mapping based on HC
pretty_palette <- c("#1f77b4", '#ff7f0e', '#2ca02c', '#d62728',
'#9467bd', '#8c564b', '#e377c2')
plot(som_model, type="mapping",label=(rownames(data)),
bgcol=pretty_palette[som_cluster], col=colors[row_label])
add.cluster.boundaries(som_model,som_cluster)
data = read.csv2("data.csv", header = TRUE, stringsAsFactors = F, dec = ".")
# For plotting evaluation against colorcode # category (~ classification solution)
row_label <- as.factor(rownames(data))
colors <- c("red", "black", "blue")
colors <- colors[as.numeric(iris$Species)]
data_train_matrix <- as.matrix(scale(data))
plot(som_model, type="codes")
##### Clustering #####
# Clustering can be performed on the SOM nodes to isolate groups of samples with similar metrics. Manual identification
# of clusters is completed by exploring the heatmaps for a number of variables and drawing up a “story” about the different
# areas on the map. An estimate of the number of clusters that would be suitable can be ascertained using a kmeans algorithm
# and examing for an “elbow-point” in the plot of “within cluster sum of squares”.  The Kohonen package documentation shows
# how a map can be clustered using hierachical clustering. The results of the clustering can be visualised using the SOM plot function again.
mydata <- som_model$codes
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
##### Clustering #####
# Clustering can be performed on the SOM nodes to isolate groups of samples with similar metrics. Manual identification
# of clusters is completed by exploring the heatmaps for a number of variables and drawing up a “story” about the different
# areas on the map. An estimate of the number of clusters that would be suitable can be ascertained using a kmeans algorithm
# and examing for an “elbow-point” in the plot of “within cluster sum of squares”.  The Kohonen package documentation shows
# how a map can be clustered using hierachical clustering. The results of the clustering can be visualised using the SOM plot function again.
mydata <- som_model$codes
data = read.csv2("data.csv", header = TRUE, stringsAsFactors = F, dec = ".")
library(kohonen)
# For plotting evaluation against colorcode # category (~ classification solution)
row_label <- as.factor(rownames(data))
colors <- c("red", "black", "blue")
colors <- colors[as.numeric(iris$Species)]
data_train_matrix <- as.matrix(scale(data))
##### Define the neuronal grid #####
som_grid <- somgrid(xdim = 4, ydim = 4, topo="hexagonal")
##### Train the model #####
som_model <- som(data_train_matrix, grid=som_grid, rlen=1000, alpha=c(0.05,0.01), keep.data=TRUE)
data_train_matrix <- as.matrix(scale(data))
colors <- colors[as.numeric(data$Item)]
data_train_matrix <- as.matrix(scale(data))
# For plotting evaluation against colorcode # category (~ classification solution)
row_label <- as.factor(rownames(data))
colors <- c("red", "black", "blue")
colors <- colors[as.numeric(data$Item)]
data_raw = read.csv2("data.csv", header = TRUE, stringsAsFactors = F, dec = ".")
# For plotting evaluation against colorcode # category (~ classification solution)
row_label <- as.factor(rownames(data_raw))
colors <- c("red", "black", "blue")
colors <- colors[as.numeric(data_raw$Item)]
# Removing duplicates in data #
data<-unique(data_raw)
# For plotting evaluation against colorcode # category (~ classification solution)
row_label <- as.factor(rownames(data))
colors <- c("red", "black", "blue")
colors <- colors[as.numeric(data$Item)]
View(data)
data_raw = read.c
# Removing duplicates in data #
data<-unique(data_raw)
# For plotting evaluation against colorcode # category (~ classification solution)
row_label <- as.factor(rownames(data))
colors <- c("red", "black", "blue")
colors <- colors[as.numeric(data$Item)]
raw = read.csv2("data.csv", header = TRUE, stringsAsFactors = F, dec = ".")
data = raw[,3:length(raw)]
rownames(data) = raw[,1]
# For plotting evaluation against colorcode # category (~ classification solution)
row_label <- as.factor(rownames(data))
colors <- c("red", "black", "blue")
colors <- colors[as.numeric(data$Item)]
data_train_matrix <- as.matrix(scale(data))
##### Define the neuronal grid #####
som_grid <- somgrid(xdim = 4, ydim = 4, topo="hexagonal")
##### Train the model #####
som_model <- som(data_train_matrix, grid=som_grid, rlen=1000, alpha=c(0.05,0.01), keep.data=TRUE)
##### Check training progress #####
# As the SOM training iterations progress, the distance from each node’s weights to the samples represented by
# that node is reduced. Ideally, this distance should reach a minimum plateau. This plot option shows the progress
# over time. If the curve is continually decreasing, more iterations are required.
plot(som_model, type="changes")
##### Node Counts #####
# The Kohonen packages allows us to visualise the count of how many samples are mapped to each node on the map.
# This metric can be used as a measure of map quality – ideally the sample distribution is relatively uniform.
# Large values in some map areas suggests that a larger map would be benificial. Empty nodes indicate that your map
# size is too big for the number of samples. Aim for at least 5-10 samples per node when choosing map size.
plot(som_model, type = "count")
plot(som_model,type = "mapping",
col=colors[row_label])
plot(som_model, type ="mapping",
labels = (rownames(data)),
col=colors[row_label])
plot(som_model, type = "property", property = getCodes(som_model, 1)[,2]) # → Make a loop over all the variables ?
plot(som_model, type="codes")
plot(som_model, type = "property", property = som_model$codes[,4], main=names(som_model$data)[4], palette.name=coolBlueHotRed)
plot(som_model, type = "property", property = som_model$codes[,4], main=names(som_model$data)[4], palette.name=colors)
plot(som_model, type = "property", property = som_model$codes[,4], main=names(som_model$data)[4], palette.name=colors)
plot(som_model, type = "property", property = som_model$codes, main=names(som_model$data), palette.name=colors)
plot(som_model, type = "property", property = som_model$codes, main=names(som_model$data))
plot(som_model, type = "property", property = som_model$codes, main=names(som_model$data))
View(data)
View(data)
plot(som_model, type = "property", property = som_model$codes, main=names(som_model$data))
plot(som_model, type = "property", property = som_model, main=names(som_model$data))
plot(som_model, type = "property", property = som_model$codes, main=names(som_model$data))
##### Clustering #####
# Clustering can be performed on the SOM nodes to isolate groups of samples with similar metrics. Manual identification
# of clusters is completed by exploring the heatmaps for a number of variables and drawing up a “story” about the different
# areas on the map. An estimate of the number of clusters that would be suitable can be ascertained using a kmeans algorithm
# and examing for an “elbow-point” in the plot of “within cluster sum of squares”.  The Kohonen package documentation shows
# how a map can be clustered using hierachical clustering. The results of the clustering can be visualised using the SOM plot function again.
mydata <- som_model$codes
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) {
wss[i] <- sum(kmeans(mydata, centers=i)$withinss)
}
plot(wss)
## use hierarchical clustering to cluster the codebook vectors
som_cluster <- cutree(hclust(dist(som_model$codes)), 6)
# plot these results:
plot(som_model, type="mapping", bgcol = pretty_palette[som_cluster], main = "Clusters")
add.cluster.boundaries(som_model, som_cluster)
install.packages("Rtsne")
data_raw = read.csv2("data.csv", header = TRUE, stringsAsFactors = F, dec = ".")
library(kohonen)
# Preparing data #
data<-unique(data_raw)
raw = read.csv2("data.csv", header = TRUE, stringsAsFactors = F, dec = ".")
data = raw[,3:length(raw)]
rownames(data) = raw[,1]
# For plotting evaluation against colorcode # category (~ classification solution)
row_label <- as.factor(rownames(data))
colors <- c("red", "black", "blue")
colors <- colors[as.numeric(data$Item)]
data_train_matrix <- as.matrix(scale(data))
##### Define the neuronal grid #####
som_grid <- somgrid(xdim = 4, ydim = 4, topo="hexagonal")
##### Train the model #####
som_model <- som(data_train_matrix, grid=som_grid, rlen=1000, alpha=c(0.05,0.01), keep.data=TRUE)
##### Check training progress #####
# As the SOM training iterations progress, the distance from each node's weights to the samples represented by
# that node is reduced. Ideally, this distance should reach a minimum plateau. This plot option shows the progress
# over time. If the curve is continually decreasing, more iterations are required.
plot(som_model, type="changes")
##### Node Counts #####
# The Kohonen packages allows us to visualise the count of how many samples are mapped to each node on the map.
# This metric can be used as a measure of map quality - ideally the sample distribution is relatively uniform.
# Large values in some map areas suggests that a larger map would be benificial. Empty nodes indicate that your map
# size is too big for the number of samples. Aim for at least 5-10 samples per node when choosing map size.
plot(som_model, type = "count")
plot(som_model,type = "mapping",
col=colors[row_label])
plot(som_model, type ="mapping",
labels = (rownames(data)),
col=colors[row_label])
plot(som_model, type = "property", property = getCodes(som_model, 1)[,2]) # ??? Make a loop over all the variables ?
plot(som_model, type="codes")
plot(som_model, type = "property", property = som_model$codes, main=names(som_model$data))
plot(som_model, type = "property", property = getCodes(som_model)[,4], main=colnames(getCodes(som_model))[4], palette.name=coolBlueHotRed)
plot(som_model, type = "property", property = getCodes(som_model)[,4], main=colnames(getCodes(som_model))[4], palette.name=coolBlueHotRed)
plot(som_model, type = "property", property = getCodes(som_model), main=colnames(getCodes(som_model)), palette.name=coolBlueHotRed)
plot(som_model, type = "property", property = getCodes(som_model), main=colnames(getCodes(som_model)))
plot(som_model, type = "property", property = getCodes(som_model), main=colnames(getCodes(som_model)))
plot(som_model, type = "property", property = som_model$codes, main=names(som_model$data))
plot(som_model, type = "property", property = getCodes(som_model), main=colnames(getCodes(som_model)))
plot(som_model, type = "property", property = getCodes(som_model), main=names(som_model$data))
##### Clustering #####
# Clustering can be performed on the SOM nodes to isolate groups of samples with similar metrics. Manual identification
# of clusters is completed by exploring the heatmaps for a number of variables and drawing up a "story" about the different
# areas on the map. An estimate of the number of clusters that would be suitable can be ascertained using a kmeans algorithm
# and examing for an "elbow-point" in the plot of "within cluster sum of squares".  The Kohonen package documentation shows
# how a map can be clustered using hierachical clustering. The results of the clustering can be visualised using the SOM plot function again.
mydata <- som_model$codes
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
wss[i] <- sum(kmeans(mydata, centers=i)$withinss)
for (i in 2:15) {
wss[i] <- sum(kmeans(mydata, centers=i)$withinss)
}
##### Clustering #####
# Clustering can be performed on the SOM nodes to isolate groups of samples with similar metrics. Manual identification
# of clusters is completed by exploring the heatmaps for a number of variables and drawing up a "story" about the different
# areas on the map. An estimate of the number of clusters that would be suitable can be ascertained using a kmeans algorithm
# and examing for an "elbow-point" in the plot of "within cluster sum of squares".  The Kohonen package documentation shows
# how a map can be clustered using hierachical clustering. The results of the clustering can be visualised using the SOM plot function again.
tree <- as.dendrogram(hclust(dist(as.numeric(unlist(som_model$codes))))) plot(tree, ylab = "Height (h)")
##### Clustering #####
# Clustering can be performed on the SOM nodes to isolate groups of samples with similar metrics. Manual identification
# of clusters is completed by exploring the heatmaps for a number of variables and drawing up a "story" about the different
# areas on the map. An estimate of the number of clusters that would be suitable can be ascertained using a kmeans algorithm
# and examing for an "elbow-point" in the plot of "within cluster sum of squares".  The Kohonen package documentation shows
# how a map can be clustered using hierachical clustering. The results of the clustering can be visualised using the SOM plot function again.
tree <- as.dendrogram(hclust(dist(as.numeric(unlist(som_model$codes)))))
plot(tree, ylab = "Height (h)")
## use hierarchical clustering to cluster the codebook vectors
som_cluster <- cutree(hclust(dist(as.numeric(unlist(som_model$codes)))), h=2
# plot these results:
plot(som_model, type="mapping", bgcol = pretty_palette[som_cluster], main = "Clusters")
## use hierarchical clustering to cluster the codebook vectors
som_cluster <- cutree(hclust(dist(as.numeric(unlist(som_model$codes)))), h=2
## use hierarchical clustering to cluster the codebook vectors
som_cluster <- cutree(hclust(dist(as.numeric(unlist(som_model$codes)))), h=2)
## use hierarchical clustering to cluster the codebook vectors
som_cluster <- cutree(hclust(dist(as.numeric(unlist(som_model$codes)))), h=2)
pretty_palette <- c("#1f77b4", '#ff7f0e', '#2ca02c', '#d62728',
'#9467bd', '#8c564b', '#e377c2')
plot(som_model, type="mapping",labels = (rownames(data)),
bgcol=pretty_palette[som_cluster], col=colors[row_label])
add.cluster.boundaries(som_model,som_cluster)
